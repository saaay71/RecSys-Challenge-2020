{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "\n",
    "@F.udf(\"String\")\n",
    "def decode_tokens(tokens):\n",
    "  return tokenizer.decode(tokens)\n",
    "\n",
    "sqc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "root_file_path = \"/PATH/temp/\"\n",
    "training_parquet_path = root_file_path+\"training_df\"\n",
    "validation_parquet_path = root_file_path+\"val_df\"\n",
    "test_parquet_path = root_file_path+\"test_df\"\n",
    "\n",
    "training_df = sqc.read.parquet(training_parquet_path)\n",
    "validation_df = sqc.read.parquet(validation_parquet_path)\n",
    "test_df = sqc.read.parquet(test_parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "training_data = training_df\\\n",
    ".withColumn(\"reply\", F.when(F.col(\"reply_engagement_timestamp\").isNull(), 0).otherwise(1))\\\n",
    ".withColumn(\"retweet\", F.when(F.col(\"retweet_engagement_timestamp\").isNull(), 0).otherwise(1))\\\n",
    ".withColumn(\"rtWithCmt\", F.when(F.col(\"retweet_with_comment_engagement_timestamp\").isNull(), 0).otherwise(1))\\\n",
    ".withColumn(\"like\", F.when(F.col(\"like_engagement_timestamp\").isNull(), 0).otherwise(1))\\\n",
    ".select(\"tweet_id\", \"engager_user_id\", \"engagee_user_id\", \"reply\", \"retweet\", \"rtWithCmt\", \"like\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "training_data = training_df\\\n",
    ".withColumn(\"reply\", F.when(F.col(\"reply_engagement_timestamp\").isNull(), 0).otherwise(1))\\\n",
    ".withColumn(\"retweet\", F.when(F.col(\"retweet_engagement_timestamp\").isNull(), 0).otherwise(1))\\\n",
    ".withColumn(\"rtWithCmt\", F.when(F.col(\"retweet_with_comment_engagement_timestamp\").isNull(), 0).otherwise(1))\\\n",
    ".withColumn(\"like\", F.when(F.col(\"like_engagement_timestamp\").isNull(), 0).otherwise(1))\\\n",
    ".select(\"tweet_id\", \"engager_user_id\", \"engagee_user_id\", \"reply\", \"retweet\", \"rtWithCmt\", \"like\")\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "create_tweet_features = PipelineModel.load(root_file_path+\"create_tweet_features_model\")\n",
    "create_engager_user_features = PipelineModel.load(root_file_path+\"create_engager_user_features_model\")\n",
    "create_engagee_user_features = PipelineModel.load(root_file_path+\"create_engagee_user_features_model\")\n",
    "\n",
    "tweet_features = sqc.read.parquet(root_file_path+\"training_tweets\")\n",
    "tweet_features = create_tweet_features.transform(tweet_features).select(\"tweet_id\", \"tweet_features\")\n",
    "\n",
    "engager_features = sqc.read.parquet(root_file_path+\"training_engager_user_df.parquet\")\n",
    "engager_features = create_engager_user_features.transform(engager_features).select(\"engager_user_id\", \"engager_features\")\n",
    "\n",
    "engagee_features = sqc.read.parquet(root_file_path+\"training_engagee_user_df.parquet\")\n",
    "engagee_features = create_engagee_user_features.transform(engagee_features).select(\"engagee_user_id\", \"engagee_features\")\n",
    "\n",
    "training_data = training_data.join(tweet_features, \"tweet_id\")\n",
    "training_data = training_data.join(engager_features, \"engager_user_id\")\n",
    "training_data = training_data.join(engagee_features, \"engagee_user_id\")\n",
    "# training_data.show()\n",
    "\n",
    "assemblerInputs = [\"tweet_features\", \"engager_features\", \"engagee_features\"]\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"featuresAssembled\")\n",
    "\n",
    "training_data = assembler.transform(training_data).drop(*assemblerInputs)\n",
    "training_data.write.parquet(root_file_path+\"training_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = sqc.read.parquet(root_file_path+\"training_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train for Likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt_like_model = GBTClassifier(labelCol=\"like\", featuresCol=\"featuresAssembled\").fit(training_data)\n",
    "gbt_like_model.save(root_file_path+\"models/gbt_like2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.ml.classification import GBTClassificationModel\n",
    "like_model_loaded = GBTClassificationModel.load(root_file_path+\"models/gbt_like\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train for Reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt_reply_model = GBTClassifier(labelCol=\"reply\", featuresCol=\"featuresAssembled\").fit(training_data)\n",
    "gbt_reply_model.save(root_file_path+\"models/gbt_reply2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply_model_loaded = GBTClassificationModel.load(root_file_path+\"models/gbt_reply\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train for Retweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt_retweet_model = GBTClassifier(labelCol=\"retweet\", featuresCol=\"featuresAssembled\").fit(training_data)\n",
    "gbt_retweet_model.save(root_file_path+\"models/gbt_retweet2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "retweet_model_loaded = GBTClassificationModel.load(root_file_path+\"models/gbt_retweet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train for RTwithCmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt_rtWithCmt_model = GBTClassifier(labelCol=\"rtWithCmt\", featuresCol=\"featuresAssembled\").fit(training_data)\n",
    "gbt_rtWithCmt_model.save(root_file_path+\"models/gbt_rtWithCmt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "retwithCmt_model_loaded = GBTClassificationModel.load(root_file_path+\"models/gbt_rtWithCmt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "validation_data = validation_df\\\n",
    ".select(\"tweet_id\", \"engager_user_id\", \"engagee_user_id\")\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "create_tweet_features = PipelineModel.load(root_file_path+\"create_tweet_features_model\")\n",
    "create_engager_user_features = PipelineModel.load(root_file_path+\"create_engager_user_features_model\")\n",
    "create_engagee_user_features = PipelineModel.load(root_file_path+\"create_engagee_user_features_model\")\n",
    "\n",
    "validation_features = sqc.read.parquet(root_file_path+\"validation_tweets\")\n",
    "validation_features = create_tweet_features.transform(validation_features).select(\"tweet_id\", \"tweet_features\")\n",
    "\n",
    "engager_features = sqc.read.parquet(root_file_path+\"validation_engager_user_df.parquet\")\n",
    "engager_features = create_engager_user_features.transform(engager_features).select(\"engager_user_id\", \"engager_features\")\n",
    "\n",
    "engagee_features = sqc.read.parquet(root_file_path+\"validation_engagee_user_df.parquet\")\n",
    "engagee_features = create_engagee_user_features.transform(engagee_features).select(\"engagee_user_id\", \"engagee_features\")\n",
    "\n",
    "validation_data = validation_data.join(validation_features, \"tweet_id\")\n",
    "validation_data = validation_data.join(engager_features, \"engager_user_id\")\n",
    "validation_data = validation_data.join(engagee_features, \"engagee_user_id\")\n",
    "# validation_data.show()\n",
    "\n",
    "assemblerInputs = [\"tweet_features\", \"engager_features\", \"engagee_features\"]\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"featuresAssembled\")\n",
    "\n",
    "validation_data = assembler.transform(validation_data).drop(*assemblerInputs)\n",
    "validation_data.write.parquet(root_file_path+\"validation_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = sqc.read.parquet(root_file_path+\"validation_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict for like on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "validation_data = sqc.read.parquet(root_file_path+\"validation_data.parquet\")\n",
    "\n",
    "print(\"making predictions ...\")\n",
    "predictions = gbt_like_model.transform(validation_data)\n",
    "\n",
    "print(\"writring predictions to file\")\n",
    "split1_udf = F.udf(lambda value: value[1].item(), T.DoubleType())\n",
    "predictions.select(\"tweet_id\", \"engagee_user_id\", split1_udf(\"probability\").alias(\"probability\"))\\\n",
    ".coalesce(1).write.csv(root_file_path+\"validaiton_predictions/like2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict for Reply on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "print(\"making predictions ...\")\n",
    "predictions = gbt_reply_model.transform(validation_data)\n",
    "\n",
    "print(\"writring predictions to file\")\n",
    "split1_udf = F.udf(lambda value: value[1].item(), T.DoubleType())\n",
    "predictions.select(\"tweet_id\", \"engagee_user_id\", split1_udf(\"probability\").alias(\"probability\"))\\\n",
    ".coalesce(1).write.csv(root_file_path+\"validaiton_predictions/reply2.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict for retweet on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "print(\"making predictions ...\")\n",
    "predictions = gbt_retweet_model.transform(validation_data)\n",
    "\n",
    "print(\"writring predictions to file\")\n",
    "split1_udf = F.udf(lambda value: value[1].item(), T.DoubleType())\n",
    "predictions.select(\"tweet_id\", \"engagee_user_id\", split1_udf(\"probability\").alias(\"probability\"))\\\n",
    ".coalesce(1).write.csv(root_file_path+\"validaiton_predictions/retweet2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict for rtWithCmt on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "print(\"making predictions ...\")\n",
    "predictions = gbt_rtWithCmt_model.transform(validation_data)\n",
    "\n",
    "print(\"writring predictions to file\")\n",
    "split1_udf = F.udf(lambda value: value[1].item(), T.DoubleType())\n",
    "predictions.select(\"tweet_id\", \"engagee_user_id\", split1_udf(\"probability\").alias(\"probability\"))\\\n",
    ".coalesce(1).write.csv(root_file_path+\"validaiton_predictions/rtWithCmt2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "test_data = test_df\\\n",
    ".select(\"tweet_id\", \"engager_user_id\", \"engagee_user_id\")\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "create_tweet_features = PipelineModel.load(root_file_path+\"create_tweet_features_model\")\n",
    "create_engager_user_features = PipelineModel.load(root_file_path+\"create_engager_user_features_model\")\n",
    "create_engagee_user_features = PipelineModel.load(root_file_path+\"create_engagee_user_features_model\")\n",
    "\n",
    "test_features = sqc.read.parquet(root_file_path+\"test_tweets\")\n",
    "test_features = create_tweet_features.transform(test_features).select(\"tweet_id\", \"tweet_features\")\n",
    "\n",
    "engager_features = sqc.read.parquet(root_file_path+\"test_engager_user_df.parquet\")\n",
    "engager_features = create_engager_user_features.transform(engager_features).select(\"engager_user_id\", \"engager_features\")\n",
    "\n",
    "engagee_features = sqc.read.parquet(root_file_path+\"test_engagee_user_df.parquet\")\n",
    "engagee_features = create_engagee_user_features.transform(engagee_features).select(\"engagee_user_id\", \"engagee_features\")\n",
    "\n",
    "test_data = test_data.join(test_features, \"tweet_id\")\n",
    "test_data = test_data.join(engager_features, \"engager_user_id\")\n",
    "test_data = test_data.join(engagee_features, \"engagee_user_id\")\n",
    "# test_data.show()\n",
    "\n",
    "assemblerInputs = [\"tweet_features\", \"engager_features\", \"engagee_features\"]\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"featuresAssembled\")\n",
    "\n",
    "test_data = assembler.transform(test_data).drop(*assemblerInputs)\n",
    "test_data.write.parquet(root_file_path+\"test_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "test_data = sqc.read.parquet(root_file_path+\"test_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict for like on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "print(\"making predictions ...\")\n",
    "predictions = gbt_like_model.transform(test_data)\n",
    "\n",
    "print(\"writring predictions to file\")\n",
    "split1_udf = F.udf(lambda value: value[1].item(), T.DoubleType())\n",
    "predictions.select(\"tweet_id\", \"engagee_user_id\", split1_udf(\"probability\").alias(\"probability\"))\\\n",
    ".coalesce(1).write.csv(root_file_path+\"predictions/like2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict for Reply on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "print(\"making predictions ...\")\n",
    "predictions = gbt_reply_model.transform(test_data)\n",
    "\n",
    "print(\"writring predictions to file\")\n",
    "split1_udf = F.udf(lambda value: value[1].item(), T.DoubleType())\n",
    "predictions.select(\"tweet_id\", \"engagee_user_id\", split1_udf(\"probability\").alias(\"probability\"))\\\n",
    ".coalesce(1).write.csv(root_file_path+\"predictions/reply2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict for rtWithCmt on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "print(\"making predictions ...\")\n",
    "predictions = gbt_rtWithCmt_model.transform(test_data)\n",
    "\n",
    "print(\"writring predictions to file\")\n",
    "split1_udf = F.udf(lambda value: value[1].item(), T.DoubleType())\n",
    "predictions.select(\"tweet_id\", \"engagee_user_id\", split1_udf(\"probability\").alias(\"probability\"))\\\n",
    ".coalesce(1).write.csv(root_file_path+\"predictions/rtWithCmt2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict for retweet on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "print(\"making predictions ...\")\n",
    "predictions = gbt_retweet_model.transform(test_data)\n",
    "\n",
    "print(\"writring predictions to file\")\n",
    "split1_udf = F.udf(lambda value: value[1].item(), T.DoubleType())\n",
    "predictions.select(\"tweet_id\", \"engagee_user_id\", split1_udf(\"probability\").alias(\"probability\"))\\\n",
    ".coalesce(1).write.csv(root_file_path+\"predictions/retweet2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
