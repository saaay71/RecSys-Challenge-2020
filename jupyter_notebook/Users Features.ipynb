{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "root_file_path = \"/PATH/temp/\"\n",
    "training_parquet_path = root_file_path+\"training_df\"\n",
    "validation_parquet_path = root_file_path+\"val_df\"\n",
    "test_parquet_path = root_file_path+\"test_df\"\n",
    "\n",
    "training_df = sqc.read.parquet(training_parquet_path)\n",
    "validation_df = sqc.read.parquet(validation_parquet_path)\n",
    "test_df = sqc.read.parquet(test_parquet_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_engager_user_df = training_df\\\n",
    ".dropDuplicates([\"engager_user_id\"])\\\n",
    ".select(\"engager_user_id\", \"engager_follower_count\", \"engager_following_count\", \"engager_is_verified\", \"engager_account_creation_time\")\\\n",
    ".withColumn(\"engager_is_verified\", F.col(\"engager_is_verified\").cast(T.IntegerType()))\\\n",
    ".withColumn(\"year_engager\", F.hour(F.to_timestamp(\"engager_account_creation_time\")))\\\n",
    ".withColumn(\"is_bot_engager\", F.when(((F.col(\"engager_following_count\") < 4700) | (F.col(\"engager_following_count\") > 5100)), 0).otherwise(1))\n",
    "\n",
    "training_engager_user_df.write.parquet(root_file_path+\"training_engager_user_df.parquet\")\n",
    "\n",
    "training_engagee_user_df = training_df\\\n",
    ".dropDuplicates([\"engagee_user_id\"])\\\n",
    ".select(\"engagee_user_id\", \"engagee_follower_count\", \"engagee_following_count\", \"engagee_is_verified\", \"engagee_account_creation_time\")\\\n",
    ".withColumn(\"engagee_is_verified\", F.col(\"engagee_is_verified\").cast(T.IntegerType()))\\\n",
    ".withColumn(\"year_engagee\", F.hour(F.to_timestamp(\"engagee_account_creation_time\")))\\\n",
    ".withColumn(\"is_bot_engagee\", F.when(((F.col(\"engagee_following_count\") < 4700) | (F.col(\"engagee_following_count\") > 5100)), 0).otherwise(1))\n",
    "\n",
    "training_engagee_user_df.write.parquet(root_file_path+\"training_engagee_user_df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "validation_engager_user_df = validation_df\\\n",
    ".dropDuplicates([\"engager_user_id\"])\\\n",
    ".select(\"engager_user_id\", \"engager_follower_count\", \"engager_following_count\", \"engager_is_verified\", \"engager_account_creation_time\")\\\n",
    ".withColumn(\"engager_is_verified\", F.col(\"engager_is_verified\").cast(T.IntegerType()))\\\n",
    ".withColumn(\"year_engager\", F.hour(F.to_timestamp(\"engager_account_creation_time\")))\\\n",
    ".withColumn(\"is_bot_engager\", F.when(((F.col(\"engager_following_count\") < 4700) | (F.col(\"engager_following_count\") > 5100)), 0).otherwise(1))\n",
    "\n",
    "validation_engager_user_df.write.parquet(root_file_path+\"validation_engager_user_df.parquet\")\n",
    "\n",
    "validation_engagee_user_df = validation_df\\\n",
    ".dropDuplicates([\"engagee_user_id\"])\\\n",
    ".select(\"engagee_user_id\", \"engagee_follower_count\", \"engagee_following_count\", \"engagee_is_verified\", \"engagee_account_creation_time\")\\\n",
    ".withColumn(\"engagee_is_verified\", F.col(\"engagee_is_verified\").cast(T.IntegerType()))\\\n",
    ".withColumn(\"year_engagee\", F.hour(F.to_timestamp(\"engagee_account_creation_time\")))\\\n",
    ".withColumn(\"is_bot_engagee\", F.when(((F.col(\"engagee_following_count\") < 4700) | (F.col(\"engagee_following_count\") > 5100)), 0).otherwise(1))\n",
    "\n",
    "validation_engagee_user_df.write.parquet(root_file_path+\"validation_engagee_user_df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "test_engager_user_df = test_df\\\n",
    ".dropDuplicates([\"engager_user_id\"])\\\n",
    ".select(\"engager_user_id\", \"engager_follower_count\", \"engager_following_count\", \"engager_is_verified\", \"engager_account_creation_time\")\\\n",
    ".withColumn(\"engager_is_verified\", F.col(\"engager_is_verified\").cast(T.IntegerType()))\\\n",
    ".withColumn(\"year_engager\", F.hour(F.to_timestamp(\"engager_account_creation_time\")))\\\n",
    ".withColumn(\"is_bot_engager\", F.when(((F.col(\"engager_following_count\") < 4700) | (F.col(\"engager_following_count\") > 5100)), 0).otherwise(1))\n",
    "\n",
    "test_engager_user_df.write.parquet(root_file_path+\"test_engager_user_df.parquet\")\n",
    "\n",
    "test_engagee_user_df = test_df\\\n",
    ".dropDuplicates([\"engagee_user_id\"])\\\n",
    ".select(\"engagee_user_id\", \"engagee_follower_count\", \"engagee_following_count\", \"engagee_is_verified\", \"engagee_account_creation_time\")\\\n",
    ".withColumn(\"engagee_is_verified\", F.col(\"engagee_is_verified\").cast(T.IntegerType()))\\\n",
    ".withColumn(\"year_engagee\", F.hour(F.to_timestamp(\"engagee_account_creation_time\")))\\\n",
    ".withColumn(\"is_bot_engagee\", F.when(((F.col(\"engagee_following_count\") < 4700) | (F.col(\"engagee_following_count\") > 5100)), 0).otherwise(1))\n",
    "\n",
    "test_engagee_user_df.write.parquet(root_file_path+\"test_engagee_user_df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engager Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.ml.feature import QuantileDiscretizer, OneHotEncoderEstimator, VectorAssembler, StringIndexer, FeatureHasher\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# engager_user_df = training_df.dropDuplicates([\"engager_user_id\"]).select(\"engager_user_id\", \"engager_follower_count\", \"engager_following_count\", \"engager_is_verified\", \"engager_account_creation_time\")\n",
    "# engager_user_df = engager_user_df.withColumn(\"year_engager\", F.hour(F.to_timestamp(\"engager_account_creation_time\")))\n",
    "\n",
    "stages = [] # stages in our Pipeline\n",
    "numericalColumns = [\"engager_follower_count\", \"engager_following_count\"]\n",
    "for numericalCol in numericalColumns:\n",
    "    # stages += [QuantileDiscretizer(numBuckets=50, handleInvalid=\"keep\", inputCol=numericalCol, outputCol=numericalCol + \"Bucket\")]\n",
    "    qd = QuantileDiscretizer(numBuckets=50, handleInvalid=\"keep\", inputCol=numericalCol, outputCol=numericalCol + \"Bucket\")\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[qd.getOutputCol()], outputCols=[numericalCol + \"classVec\"])\n",
    "    stages += [qd, encoder]\n",
    "\n",
    "# stages = [] # stages in second Pipeline\n",
    "categoricalColumns = [\"year_engager\", \"engager_is_verified\", \"is_bot_engager\"]\n",
    "for categoricalCol in categoricalColumns:\n",
    "    # Category Indexing with StringIndexer\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "    # encoder = OneHotEncoderEstimator(inputCol=categoricalCol + \"Index\", outputCol=categoricalCol + \"classVec\")\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    # Add stages.  These are not run here, but will run all at once later on.\n",
    "    stages += [stringIndexer, encoder]\n",
    "    \n",
    "# pipeline2 = Pipeline(stages=stages)\n",
    "\n",
    "# assemblerInputs = [\"engager_follower_countclassVec\", \"engager_following_countclassVec\", \"year_engagerclassVec\", \"engager_is_verifiedclassVec\", \"is_bot_engagerclassVec\"]\n",
    "# assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"featuresAssembled\")\n",
    "# stages += [assembler]\n",
    "\n",
    "featInputs = [c + \"Bucket\" for c in numericalColumns]\n",
    "featInputs += [\"year_engagerIndex\", \"engager_is_verifiedIndex\", \"is_bot_engager\"]\n",
    "stages += [FeatureHasher(numFeatures=16, inputCols=featInputs, outputCol=\"engager_features\", categoricalCols=featInputs)]\n",
    "\n",
    "\n",
    "engager_user_features = Pipeline(stages=stages).fit(training_engager_user_df)\n",
    "engager_user_features.transform(training_engager_user_df).select(\"engager_user_id\", \"engager_features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engagee Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.ml.feature import QuantileDiscretizer, OneHotEncoderEstimator, VectorAssembler, FeatureHasher\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# engager_user_df = training_df.dropDuplicates([\"engager_user_id\"]).select(\"engager_user_id\", \"engager_follower_count\", \"engager_following_count\", \"engager_is_verified\", \"engager_account_creation_time\")\n",
    "# engager_user_df = engager_user_df.withColumn(\"year_engager\", F.hour(F.to_timestamp(\"engager_account_creation_time\")))\n",
    "\n",
    "stages = [] # stages in our Pipeline\n",
    "numericalColumns = [\"engagee_follower_count\", \"engagee_following_count\"]\n",
    "for numericalCol in numericalColumns:\n",
    "    # stages += [QuantileDiscretizer(numBuckets=50, handleInvalid=\"keep\", inputCol=numericalCol, outputCol=numericalCol + \"Bucket\")]\n",
    "    qd = QuantileDiscretizer(numBuckets=50, handleInvalid=\"keep\", inputCol=numericalCol, outputCol=numericalCol + \"Bucket\")\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[qd.getOutputCol()], outputCols=[numericalCol + \"classVec\"])\n",
    "    stages += [qd, encoder]\n",
    "\n",
    "# stages = [] # stages in second Pipeline\n",
    "categoricalColumns = [\"year_engagee\", \"engagee_is_verified\", \"is_bot_engagee\"]\n",
    "for categoricalCol in categoricalColumns:\n",
    "    # Category Indexing with StringIndexer\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "    # encoder = OneHotEncoderEstimator(inputCol=categoricalCol + \"Index\", outputCol=categoricalCol + \"classVec\")\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    # Add stages.  These are not run here, but will run all at once later on.\n",
    "    stages += [stringIndexer, encoder]\n",
    "    \n",
    "# pipeline2 = Pipeline(stages=stages)\n",
    "\n",
    "# assemblerInputs = [\"engagee_follower_countclassVec\", \"engagee_following_countclassVec\", \"year_engageeclassVec\", \"engagee_is_verifiedclassVec\", \"is_bot_engageeclassVec\"]\n",
    "# assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"featuresAssembled\")\n",
    "# stages += [assembler]\n",
    "\n",
    "featInputs = [c + \"Bucket\" for c in numericalColumns]\n",
    "featInputs += [\"year_engageeIndex\", \"engagee_is_verifiedIndex\", \"is_bot_engageeIndex\"]\n",
    "stages += [FeatureHasher(numFeatures=16, inputCols=featInputs, outputCol=\"engagee_features\", categoricalCols=featInputs)]\n",
    "\n",
    "\n",
    "engagee_user_features = Pipeline(stages=stages).fit(training_engagee_user_df)\n",
    "engagee_user_features.transform(training_engagee_user_df).select(\"engagee_user_id\", \"engagee_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "engager_user_features.save(root_file_path+\"create_engager_user_features_model\")\n",
    "engagee_user_features.save(root_file_path+\"create_engagee_user_features_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.ml import PipelineModel\n",
    "engager_user_features = PipelineModel.load(root_file_path+\"create_engager_user_features_model\")\n",
    "engagee_user_features = PipelineModel.load(root_file_path+\"create_engagee_user_features_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create engager and engagee features on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "engager_user_features.transform(test_engager_user_df).select(\"engager_user_id\", \"engager_features\").show()\n",
    "engagee_user_features.transform(test_engagee_user_df).select(\"engagee_user_id\", \"engagee_features\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
