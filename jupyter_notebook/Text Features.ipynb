{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_file_path = \"/nas_ssd_social_media_analytics/ali_twitter/final_dataset_12062020/temp/\"\n",
    "training_parquet_path = root_file_path+\"training_df\"\n",
    "validation_parquet_path = root_file_path+\"val_df\"\n",
    "test_parquet_path = root_file_path+\"test_df\"\n",
    "\n",
    "training_df = sqc.read.parquet(training_parquet_path)\n",
    "validation_df = sqc.read.parquet(validation_parquet_path)\n",
    "test_df = sqc.read.parquet(test_parquet_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unique Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tweets = training_df.dropDuplicates([\"tweet_id\"])\\\n",
    ".select(\"tweet_id\",\n",
    "        decode_tokens(F.col(\"text_tokens\")).alias(\"text\"),\n",
    "        \"tweet_type\",\n",
    "        \"language\",\n",
    "        F.hour(F.to_timestamp(\"timestamp\")).alias(\"hour_tweet\"),\n",
    "        F.size(\"text_tokens\").alias(\"num_tokens\"),\n",
    "        F.when(F.col(\"hashtags\").isNull(), 0).otherwise(1).alias(\"has_hashtags\"),\n",
    "        F.when(F.col(\"present_media\").isNull(), 0).otherwise(1).alias(\"has_media\"), \n",
    "        F.when(F.col(\"present_links\").isNull(), 0).otherwise(1).alias(\"has_links\"))\n",
    "\n",
    "\n",
    "# training_tweets.write.parquet(root_file_path+\"training_tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_is_in_top_daily_hashtag = training_df\\\n",
    ".dropDuplicates([\"tweet_id\"])\\\n",
    ".select(\"tweet_id\", \"timestamp\", F.explode(\"hashtags\").alias(\"hashtags_exploded\"))\\\n",
    ".withColumn(\"hashtag_duration\", F.max(\"timestamp\").over(Window.partitionBy(\"hashtags_exploded\")))\\\n",
    ".withColumn(\"hashtag_duration\", F.col(\"hashtag_duration\") - F.min(\"timestamp\").over(Window.partitionBy(\"hashtags_exploded\")))\\\n",
    ".withColumn(\"hashtag_duration2\", F.col(\"hashtag_duration\") / (24*3600))\\\n",
    ".withColumn(\"hashtag_duration3\", F.round(\"hashtag_duration2\"))\\\n",
    ".withColumn(\"is_in_top_daily_hashtags\", F.when( ((F.col(\"hashtag_duration3\") > 0) & (F.abs(F.col(\"hashtag_duration3\") - F.col(\"hashtag_duration2\")) < 0.007)), 1).otherwise(0))\\\n",
    ".select(\"tweet_id\", \"is_in_top_daily_hashtags\")\\\n",
    ".groupBy(\"tweet_id\").agg(F.max(\"is_in_top_daily_hashtags\").alias(\"is_in_top_daily_hashtags\"))\n",
    "\n",
    "training_tweets = training_tweets.join(tweet_is_in_top_daily_hashtag, \"tweet_id\", \"left_outer\")\\\n",
    ".withColumn(\"is_in_top_daily_hashtags\", F.when(F.col(\"is_in_top_daily_hashtags\").isNull(), 0).otherwise(F.col(\"is_in_top_daily_hashtags\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_is_in_top_daily_link = training_df\\\n",
    ".dropDuplicates([\"tweet_id\"])\\\n",
    ".select(\"tweet_id\", \"timestamp\", F.explode(\"present_links\").alias(\"present_links_exploded\"))\\\n",
    ".withColumn(\"link_duration\", F.max(\"timestamp\").over(Window.partitionBy(\"present_links_exploded\")))\\\n",
    ".withColumn(\"link_duration\", F.col(\"link_duration\") - F.min(\"timestamp\").over(Window.partitionBy(\"present_links_exploded\")))\\\n",
    ".withColumn(\"link_duration2\", F.col(\"link_duration\") / (24*3600))\\\n",
    ".withColumn(\"link_duration3\", F.round(\"link_duration2\"))\\\n",
    ".withColumn(\"is_in_top_daily_links\", F.when( ((F.col(\"link_duration3\") > 0) & (F.abs(F.col(\"link_duration3\") - F.col(\"link_duration2\")) < 0.007)), 1).otherwise(0))\\\n",
    ".select(\"tweet_id\", \"is_in_top_daily_links\")\\\n",
    ".groupBy(\"tweet_id\").agg(F.max(\"is_in_top_daily_links\").alias(\"is_in_top_daily_links\"))\n",
    "\n",
    "training_tweets = training_tweets.join(tweet_is_in_top_daily_link, \"tweet_id\", \"left_outer\")\\\n",
    ".withColumn(\"is_in_top_daily_links\", F.when(F.col(\"is_in_top_daily_links\").isNull(), 0).otherwise(F.col(\"is_in_top_daily_links\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tweets.write.parquet(root_file_path+\"training_tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tweets = sqc.read.parquet(root_file_path+\"training_tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creatring TF-IDF vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover, HashingTF, IDF, CountVectorizer\n",
    "\n",
    "# tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "# tmp = tokenizer.transform(text_d\n",
    "\n",
    "reTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"(?U)\\\\bhttps?://\\\\S*|#?\\\\b\\\\w+\\\\b\", gaps=False)\n",
    "# tmp = reTokenizer.transform(tmp)\n",
    "\n",
    "langs = [\"english\", \"french\", \"spanish\", \"german\", \"finnish\", \"turkish\", \"english\", \"russian\", \"norwegian\", \"dutch\", \"danish\", \"hungarian\", \"italian\", \"swedish\", \"portuguese\"]\n",
    "myStopWords = []\n",
    "for i in langs:\n",
    "    myStopWords += StopWordsRemover.loadDefaultStopWords(i)\n",
    "otherWordsToRemove = [\"cls\", \"sep\", \"unk\",  \"@\", \"rt\"]\n",
    "myStopWords += otherWordsToRemove\n",
    "\n",
    "stopwordsRemover = StopWordsRemover(inputCol=reTokenizer.getOutputCol(), outputCol=\"tokens2\", stopWords=myStopWords)\n",
    "# tmp = stopwordsRemover.transform(tmp)\n",
    "\n",
    "# cv = CountVectorizer(inputCol=stopwordsRemover.getOutputCol(), outputCol=\"tf\")\n",
    "# model = cv.fit(df)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=stopwordsRemover.getOutputCol(), outputCol=\"hashedTF\", numFeatures=16)\n",
    "# tmp = hashingTF.transform(tmp)\n",
    "\n",
    "# tf.cache()\n",
    "\n",
    "IDF = IDF(inputCol=hashingTF.getOutputCol(), outputCol=\"text_features\")\n",
    "# idf = IDF().fit(tmp)\n",
    "# tfidf = idf.transform(tf)\n",
    "\n",
    "# idfIgnore = IDF(minDocFreq=2).fit(tf)\n",
    "# tfidfIgnore = idfIgnore.transform(tf)\n",
    "\n",
    "stages = [reTokenizer, stopwordsRemover, hashingTF, IDF]\n",
    "\n",
    "tf_idf_pipeline = Pipeline(stages=stages)\n",
    "# tfidf_model = tf_idf_pipeline.fit(training_tweets)\n",
    "# tfidf_model.transform(training_tweets).select(\"tweet_id\", \"text_features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler, FeatureHasher\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "categoricalColumns = [\"tweet_type\", \"language\", \"hour_tweet\", \"has_hashtags\", \"has_media\", \"has_links\", \"is_in_top_daily_hashtags\", \"is_in_top_daily_links\"]\n",
    "stages = [] # stages in our Pipeline\n",
    "for categoricalCol in categoricalColumns:\n",
    "    # Category Indexing with StringIndexer\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    stages += [stringIndexer]\n",
    "    \n",
    "# assemblerInputs = [c + \"classVec\" for c in categoricalColumns]\n",
    "# assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"featuresAssembled\")\n",
    "# stages += [assembler]\n",
    "\n",
    "featInputs = [c + \"Index\" for c in categoricalColumns]\n",
    "featHasher = FeatureHasher(numFeatures=16, inputCols=featInputs, outputCol=\"otherFeaturesHashed\", categoricalCols=featInputs)\n",
    "stages += [featHasher]\n",
    "\n",
    "other_tweet_features_pipline = Pipeline(stages=stages)\n",
    "# other_tweet_features = other_tweet_features_pipline.fit(training_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "assemble_tweet_features = VectorAssembler(inputCols=[\"text_features\", \"otherFeaturesHashed\"], outputCol=\"tweet_features\")\n",
    "create_tweet_features = Pipeline(stages=[tf_idf_pipeline, other_tweet_features_pipline, assemble_tweet_features]).fit(training_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "create_tweet_features.transform(training_tweets).select(\"tweet_id\", \"tweet_features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "create_tweet_features.save(root_file_path+\"create_tweet_features_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.ml import PipelineModel\n",
    "create_tweet_features = PipelineModel.load(root_file_path+\"create_tweet_features_model\")\n",
    "# create_tweet_features.transform(training_tweets).select(\"tweet_id\", \"tweet_features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create tweet features for validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "validation_tweets = validation_df.dropDuplicates([\"tweet_id\"])\\\n",
    ".select(\"tweet_id\",\n",
    "        decode_tokens(F.col(\"text_tokens\")).alias(\"text\"),\n",
    "        \"tweet_type\",\n",
    "        \"language\",\n",
    "        F.hour(F.to_timestamp(\"timestamp\")).alias(\"hour_tweet\"),\n",
    "        F.size(\"text_tokens\").alias(\"num_tokens\"),\n",
    "        F.when(F.col(\"hashtags\").isNull(), 0).otherwise(1).alias(\"has_hashtags\"),\n",
    "        F.when(F.col(\"present_media\").isNull(), 0).otherwise(1).alias(\"has_media\"), \n",
    "        F.when(F.col(\"present_links\").isNull(), 0).otherwise(1).alias(\"has_links\"))\n",
    "        \n",
    "tweet_is_in_top_daily_hashtag = validation_df\\\n",
    ".dropDuplicates([\"tweet_id\"])\\\n",
    ".select(\"tweet_id\", \"timestamp\", F.explode(\"hashtags\").alias(\"hashtags_exploded\"))\\\n",
    ".withColumn(\"hashtag_duration\", F.max(\"timestamp\").over(Window.partitionBy(\"hashtags_exploded\")))\\\n",
    ".withColumn(\"hashtag_duration\", F.col(\"hashtag_duration\") - F.min(\"timestamp\").over(Window.partitionBy(\"hashtags_exploded\")))\\\n",
    ".withColumn(\"hashtag_duration2\", F.col(\"hashtag_duration\") / (24*3600))\\\n",
    ".withColumn(\"hashtag_duration3\", F.round(\"hashtag_duration2\"))\\\n",
    ".withColumn(\"is_in_top_daily_hashtags\", F.when( ((F.col(\"hashtag_duration3\") > 0) & (F.abs(F.col(\"hashtag_duration3\") - F.col(\"hashtag_duration2\")) < 0.007)), 1).otherwise(0))\\\n",
    ".select(\"tweet_id\", \"is_in_top_daily_hashtags\")\\\n",
    ".groupBy(\"tweet_id\").agg(F.max(\"is_in_top_daily_hashtags\").alias(\"is_in_top_daily_hashtags\"))\n",
    "\n",
    "validation_tweets = validation_tweets.join(tweet_is_in_top_daily_hashtag, \"tweet_id\", \"left_outer\")\\\n",
    ".withColumn(\"is_in_top_daily_hashtags\", F.when(F.col(\"is_in_top_daily_hashtags\").isNull(), 0).otherwise(F.col(\"is_in_top_daily_hashtags\")))\n",
    "\n",
    "tweet_is_in_top_daily_link = validation_df\\\n",
    ".dropDuplicates([\"tweet_id\"])\\\n",
    ".select(\"tweet_id\", \"timestamp\", F.explode(\"present_links\").alias(\"present_links_exploded\"))\\\n",
    ".withColumn(\"link_duration\", F.max(\"timestamp\").over(Window.partitionBy(\"present_links_exploded\")))\\\n",
    ".withColumn(\"link_duration\", F.col(\"link_duration\") - F.min(\"timestamp\").over(Window.partitionBy(\"present_links_exploded\")))\\\n",
    ".withColumn(\"link_duration2\", F.col(\"link_duration\") / (24*3600))\\\n",
    ".withColumn(\"link_duration3\", F.round(\"link_duration2\"))\\\n",
    ".withColumn(\"is_in_top_daily_links\", F.when( ((F.col(\"link_duration3\") > 0) & (F.abs(F.col(\"link_duration3\") - F.col(\"link_duration2\")) < 0.007)), 1).otherwise(0))\\\n",
    ".select(\"tweet_id\", \"is_in_top_daily_links\")\\\n",
    ".groupBy(\"tweet_id\").agg(F.max(\"is_in_top_daily_links\").alias(\"is_in_top_daily_links\"))\n",
    "\n",
    "validation_tweets = validation_tweets.join(tweet_is_in_top_daily_link, \"tweet_id\", \"left_outer\")\\\n",
    ".withColumn(\"is_in_top_daily_links\", F.when(F.col(\"is_in_top_daily_links\").isNull(), 0).otherwise(F.col(\"is_in_top_daily_links\")))\n",
    "\n",
    "validation_tweets.write.parquet(root_file_path+\"validation_tweets\")\n",
    "\n",
    "# create_tweet_features.transform(validation_tweets).select(\"tweet_id\", \"tweet_features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create tweet features for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "test_tweets = test_df.dropDuplicates([\"tweet_id\"])\\\n",
    ".select(\"tweet_id\",\n",
    "        decode_tokens(F.col(\"text_tokens\")).alias(\"text\"),\n",
    "        \"tweet_type\",\n",
    "        \"language\",\n",
    "        F.hour(F.to_timestamp(\"timestamp\")).alias(\"hour_tweet\"),\n",
    "        F.size(\"text_tokens\").alias(\"num_tokens\"),\n",
    "        F.when(F.col(\"hashtags\").isNull(), 0).otherwise(1).alias(\"has_hashtags\"),\n",
    "        F.when(F.col(\"present_media\").isNull(), 0).otherwise(1).alias(\"has_media\"), \n",
    "        F.when(F.col(\"present_links\").isNull(), 0).otherwise(1).alias(\"has_links\"))\n",
    "        \n",
    "test_tweet_is_in_top_daily_hashtag = test_df\\\n",
    ".dropDuplicates([\"tweet_id\"])\\\n",
    ".select(\"tweet_id\", \"timestamp\", F.explode(\"hashtags\").alias(\"hashtags_exploded\"))\\\n",
    ".withColumn(\"hashtag_duration\", F.max(\"timestamp\").over(Window.partitionBy(\"hashtags_exploded\")))\\\n",
    ".withColumn(\"hashtag_duration\", F.col(\"hashtag_duration\") - F.min(\"timestamp\").over(Window.partitionBy(\"hashtags_exploded\")))\\\n",
    ".withColumn(\"hashtag_duration2\", F.col(\"hashtag_duration\") / (24*3600))\\\n",
    ".withColumn(\"hashtag_duration3\", F.round(\"hashtag_duration2\"))\\\n",
    ".withColumn(\"is_in_top_daily_hashtags\", F.when( ((F.col(\"hashtag_duration3\") > 0) & (F.abs(F.col(\"hashtag_duration3\") - F.col(\"hashtag_duration2\")) < 0.007)), 1).otherwise(0))\\\n",
    ".select(\"tweet_id\", \"is_in_top_daily_hashtags\")\\\n",
    ".groupBy(\"tweet_id\").agg(F.max(\"is_in_top_daily_hashtags\").alias(\"is_in_top_daily_hashtags\"))\n",
    "\n",
    "test_tweets = test_tweets.join(test_tweet_is_in_top_daily_hashtag, \"tweet_id\", \"left_outer\")\\\n",
    ".withColumn(\"is_in_top_daily_hashtags\", F.when(F.col(\"is_in_top_daily_hashtags\").isNull(), 0).otherwise(F.col(\"is_in_top_daily_hashtags\")))\n",
    "\n",
    "test_tweet_is_in_top_daily_link = test_df\\\n",
    ".dropDuplicates([\"tweet_id\"])\\\n",
    ".select(\"tweet_id\", \"timestamp\", F.explode(\"present_links\").alias(\"present_links_exploded\"))\\\n",
    ".withColumn(\"link_duration\", F.max(\"timestamp\").over(Window.partitionBy(\"present_links_exploded\")))\\\n",
    ".withColumn(\"link_duration\", F.col(\"link_duration\") - F.min(\"timestamp\").over(Window.partitionBy(\"present_links_exploded\")))\\\n",
    ".withColumn(\"link_duration2\", F.col(\"link_duration\") / (24*3600))\\\n",
    ".withColumn(\"link_duration3\", F.round(\"link_duration2\"))\\\n",
    ".withColumn(\"is_in_top_daily_links\", F.when( ((F.col(\"link_duration3\") > 0) & (F.abs(F.col(\"link_duration3\") - F.col(\"link_duration2\")) < 0.007)), 1).otherwise(0))\\\n",
    ".select(\"tweet_id\", \"is_in_top_daily_links\")\\\n",
    ".groupBy(\"tweet_id\").agg(F.max(\"is_in_top_daily_links\").alias(\"is_in_top_daily_links\"))\n",
    "\n",
    "test_tweets = test_tweets.join(test_tweet_is_in_top_daily_link, \"tweet_id\", \"left_outer\")\\\n",
    ".withColumn(\"is_in_top_daily_links\", F.when(F.col(\"is_in_top_daily_links\").isNull(), 0).otherwise(F.col(\"is_in_top_daily_links\")))\n",
    "\n",
    "test_tweets.write.parquet(root_file_path+\"test_tweets\")\n",
    "\n",
    "# create_tweet_features.transform(test_tweets).select(\"tweet_id\", \"tweet_features\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
